{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved YouTube Comment Bot Detection Analysis\n",
    "\n",
    "This notebook demonstrates the improved, modular architecture for bot detection analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import the improved modules\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to Python path if needed\n",
    "# sys.path.append(str(Path.cwd()))\n",
    "\n",
    "from bot_detector import BotDetector\n",
    "from config import setup_logging, analysis_config, model_config\n",
    "from visualization import BotDetectionVisualizer\n",
    "from ml_processing import CommentAnalyzer\n",
    "import polars as pl\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logging()\n",
    "print(\"Modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Setup\n",
    "\n",
    "The new architecture uses proper configuration management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Configuration:\n",
      "  Default date range: 2023-10-31 to 2024-10-31\n",
      "  Suspicious threshold: 10\n",
      "  User ID sampling: mod 100\n",
      "\n",
      "Model Configuration:\n",
      "  Model: DeepPavlov/rubert-base-cased-sentence\n",
      "  Batch size: 32\n",
      "  PCA components: 50\n",
      "  Number of clusters: 10\n"
     ]
    }
   ],
   "source": [
    "# Display current configuration\n",
    "print(\"Analysis Configuration:\")\n",
    "print(f\"  Default date range: {analysis_config.default_start_date} to {analysis_config.default_end_date}\")\n",
    "print(f\"  Suspicious threshold: {analysis_config.suspicious_threshold}\")\n",
    "print(f\"  User ID sampling: mod {analysis_config.user_id_mod}\")\n",
    "\n",
    "print(\"\\nModel Configuration:\")\n",
    "print(f\"  Model: {model_config.model_name}\")\n",
    "print(f\"  Batch size: {model_config.batch_size}\")\n",
    "print(f\"  PCA components: {model_config.pca_components}\")\n",
    "print(f\"  Number of clusters: {model_config.n_clusters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Bot Detector\n",
    "\n",
    "The main orchestrator handles all complex operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 23:44:21,258 - database - INFO - Database connection pool created successfully\n",
      "2025-09-04 23:44:21,259 - ml_processing - INFO - Loading model: DeepPavlov/rubert-base-cased-sentence\n",
      "2025-09-04 23:44:23,559 - ml_processing - INFO - Model loaded successfully\n",
      "2025-09-04 23:44:23,559 - config - INFO - Bot detector initialized successfully\n",
      "/Users/romanistomin/Documents/bot_buster/database.py:125: DataOrientationWarning: Row orientation inferred during DataFrame construction. Explicitly specify the orientation by passing `orient=\"row\"` to silence this warning.\n",
      "  return pl.DataFrame(results, schema=columns)\n",
      "2025-09-04 23:44:23,571 - config - INFO - Table youtube_comments: 10 columns\n",
      "2025-09-04 23:44:23,573 - config - INFO - Table youtube_users: 5 columns\n",
      "2025-09-04 23:44:23,588 - config - INFO - Table youtube_videos: 9 columns\n",
      "2025-09-04 23:44:23,591 - config - INFO - Table youtube_channels: 7 columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database connection validated successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize the bot detector\n",
    "detector = BotDetector()\n",
    "\n",
    "# Validate database connection\n",
    "if detector.validate_database_connection():\n",
    "    print(\"✅ Database connection validated successfully\")\n",
    "else:\n",
    "    print(\"❌ Database connection failed\")\n",
    "    # You might want to stop here if database connection fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Analyze Comment Data\n",
    "\n",
    "Load comment statistics using the optimized query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 23:44:27,431 - config - INFO - Loading comment data from 2023-10-31 to 2024-10-31\n",
      "2025-09-04 23:44:27,434 - database - INFO - Executing query from template: queries/minutely_comments_optimized.sql\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading comment data from 2023-10-31 to 2024-10-31...\n"
     ]
    }
   ],
   "source": [
    "# Load comment data - using optimized query by default\n",
    "start_date = '2023-10-31'\n",
    "end_date = '2024-10-31'\n",
    "\n",
    "print(f\"Loading comment data from {start_date} to {end_date}...\")\n",
    "comment_data = detector.load_comment_data(\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    use_optimized_query=True\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(comment_data)} records\")\n",
    "print(f\"Columns: {comment_data.columns}\")\n",
    "comment_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Identify Suspicious Commenters\n",
    "\n",
    "Use statistical analysis to identify potential bot accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify suspicious commenters\n",
    "suspicious_commenters = detector.identify_suspicious_commenters(\n",
    "    comment_data, \n",
    "    threshold=10  # Comments per period threshold\n",
    ")\n",
    "\n",
    "print(f\"Found {len(suspicious_commenters)} suspicious commenters\")\n",
    "\n",
    "# Show top suspicious commenters\n",
    "top_suspicious = (\n",
    "    suspicious_commenters\n",
    "    .sort('max_CPP_this_CH', descending=True)\n",
    "    .head(10)\n",
    "    .select(['username', 'number_comments_all_time', 'max_CPP_this_CH', 'channel_title'])\n",
    ")\n",
    "\n",
    "print(\"\\nTop 10 most suspicious commenters:\")\n",
    "top_suspicious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Basic Visualizations\n",
    "\n",
    "Use the improved visualization system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = BotDetectionVisualizer(output_dir=\"notebook_plots\")\n",
    "\n",
    "# Create cumulative distribution plot\n",
    "fig1 = visualizer.plot_cumulative_comments_distribution(comment_data)\n",
    "fig1.show()\n",
    "\n",
    "# Create suspicious commenter heatmap\n",
    "if len(suspicious_commenters) > 0:\n",
    "    fig2 = visualizer.plot_suspicious_commenter_heatmap(suspicious_commenters)\n",
    "    fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Analysis and Clustering (Optional)\n",
    "\n",
    "Analyze comment text patterns using machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load comment text for suspicious users (last month only for performance)\n",
    "text_start_date = '2024-10-01'\n",
    "text_end_date = '2024-10-31'\n",
    "\n",
    "print(\"Loading comment text for suspicious users...\")\n",
    "comments_with_text = detector.load_suspicious_comments_text(\n",
    "    suspicious_commenters,\n",
    "    start_date=text_start_date,\n",
    "    end_date=text_end_date\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(comments_with_text)} comments for text analysis\")\n",
    "if len(comments_with_text) > 0:\n",
    "    print(f\"Sample comment: {comments_with_text.select('text').head(1).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ML analysis if we have comments\n",
    "if len(comments_with_text) > 0:\n",
    "    print(\"Starting ML analysis...\")\n",
    "    \n",
    "    # This will take some time - the model needs to download and process embeddings\n",
    "    clustered_comments = detector.analyze_comment_patterns(comments_with_text)\n",
    "    \n",
    "    print(f\"Clustering completed. Found {clustered_comments.select('cluster_label').n_unique().item()} clusters\")\n",
    "    \n",
    "    # Show cluster distribution\n",
    "    cluster_dist = (\n",
    "        clustered_comments.group_by('cluster_label')\n",
    "        .agg(pl.len().alias('count'))\n",
    "        .sort('cluster_label')\n",
    "    )\n",
    "    print(\"\\nCluster distribution:\")\n",
    "    print(cluster_dist)\n",
    "else:\n",
    "    print(\"No comments available for ML analysis\")\n",
    "    clustered_comments = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Visualizations\n",
    "\n",
    "Create cluster visualizations if we have ML results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cluster visualizations if available\n",
    "if clustered_comments is not None and 'cluster_label' in clustered_comments.columns:\n",
    "    # t-SNE visualization\n",
    "    fig3 = visualizer.plot_cluster_tsne(clustered_comments)\n",
    "    fig3.show()\n",
    "    \n",
    "    # Cluster statistics\n",
    "    # First merge with original comment data to get statistics\n",
    "    enhanced_data = clustered_comments.join(\n",
    "        comment_data.select(['user_id', 'number_comments_all_time', 'max_CPP_this_CH', 'mean_likes_per_comment']),\n",
    "        on='user_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    if len(enhanced_data) > 0:\n",
    "        fig4 = visualizer.plot_cluster_statistics(enhanced_data)\n",
    "        fig4.show()\n",
    "else:\n",
    "    print(\"No cluster data available for advanced visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Examine Cluster Samples\n",
    "\n",
    "Look at actual comments from different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine sample comments from each cluster\n",
    "if clustered_comments is not None:\n",
    "    analyzer = CommentAnalyzer()\n",
    "    \n",
    "    n_clusters = clustered_comments.select('cluster_label').n_unique().item()\n",
    "    \n",
    "    for cluster_id in range(min(5, n_clusters)):  # Show first 5 clusters\n",
    "        samples = analyzer.get_cluster_samples(\n",
    "            clustered_comments, \n",
    "            cluster_id, \n",
    "            n_samples=3\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n=== Cluster {cluster_id} Samples ===\")\n",
    "        for i, comment in enumerate(samples, 1):\n",
    "            print(f\"{i}. {comment[:100]}{'...' if len(comment) > 100 else ''}\")\n",
    "else:\n",
    "    print(\"No clustered comments available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Complete Report\n",
    "\n",
    "Use the automated reporting system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "print(\"Generating comprehensive analysis report...\")\n",
    "\n",
    "report = detector.generate_report(\n",
    "    comment_data, \n",
    "    clustered_comments,\n",
    "    output_dir=\"notebook_analysis_output\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== Analysis Report Generated ===\")\n",
    "print(f\"Output directory: {report['output_directory']}\")\n",
    "print(f\"Files generated: {list(report['files_generated'].keys())}\")\n",
    "print(f\"Analysis summary: {report['analysis_summary']}\")\n",
    "\n",
    "# Display summary statistics\n",
    "summary = visualizer.create_summary_report(comment_data)\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "print(f\"Total users analyzed: {summary['total_users']:,}\")\n",
    "print(f\"Total comments: {summary['total_comments']:,}\")\n",
    "print(f\"Suspicious users: {summary['suspicious_users']} ({summary['suspicious_percentage']:.1f}%)\")\n",
    "print(f\"Average comments per user: {summary['avg_comments_per_user']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Alternative: Run Full Analysis Pipeline\n",
    "\n",
    "You can also run the entire analysis in one command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Run the complete pipeline in one go\n",
    "# Uncomment and run this cell if you want to run everything automatically\n",
    "\n",
    "\"\"\"\n",
    "# Create a new detector instance\n",
    "full_detector = BotDetector()\n",
    "\n",
    "# Run complete analysis\n",
    "full_results = full_detector.run_full_analysis(\n",
    "    start_date='2024-01-01',\n",
    "    end_date='2024-12-31',\n",
    "    include_text_analysis=True,  # Set to False to skip ML analysis for speed\n",
    "    output_dir='full_analysis_results'\n",
    ")\n",
    "\n",
    "print(\"Full analysis completed:\")\n",
    "print(full_results)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This improved analysis system provides:\n",
    "\n",
    "1. **Modular Architecture**: Separated concerns with dedicated modules\n",
    "2. **Better Performance**: Optimized SQL queries and efficient data processing\n",
    "3. **Comprehensive Validation**: Data quality checks and error handling\n",
    "4. **Security**: Environment-based configuration and SQL injection prevention\n",
    "5. **Scalability**: Connection pooling and batch processing\n",
    "6. **Maintainability**: Type hints, logging, and clear documentation\n",
    "\n",
    "The system can be used both interactively (as demonstrated here) and programmatically through the main API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
