{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romanistomin/Documents/bot_buster/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name DeepPavlov/rubert-base-cased-sentence. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 768)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# model = SentenceTransformer(\"cointegrated/rubert-tiny2\")\n",
    "model = SentenceTransformer(\"DeepPavlov/rubert-base-cased-sentence\")  # may require manual pooling\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"Привет, как дела?\",\n",
    "    \"Это тестовое предложение.\",\n",
    "    \"Я люблю машинное обучение.\"\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "print(embeddings.shape)  # e.g., (3, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.82871413\n",
      "Cosine distance: 0.17128587\n",
      "Euclidean distance: 0.5852964\n"
     ]
    }
   ],
   "source": [
    "vec1 = model.encode(\"в прошлом году я купил дом\")\n",
    "vec2 = model.encode(\"мои апартаменты куплены в прошлом году\")\n",
    "\n",
    "# ✅ Cosine similarity (0 to 1)\n",
    "cos_sim = cosine_similarity([vec1], [vec2])[0][0]\n",
    "print(\"Cosine similarity:\", cos_sim)\n",
    "\n",
    "# ✅ Cosine distance (0 = identical)\n",
    "cos_dist = 1 - cos_sim\n",
    "print(\"Cosine distance:\", cos_dist)\n",
    "\n",
    "# ✅ Euclidean distance\n",
    "euclid_dist = np.linalg.norm(vec1 - vec2)\n",
    "print(\"Euclidean distance:\", euclid_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load model + tokenizer\n",
    "model_name = \"DeepPavlov/rubert-base-cased-sentence\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Input sentence(s)\n",
    "sentences = [\"в прошлом году я купил дом\", \"с недавних пор у меня есть недвижимость\"]\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    token_embeddings = outputs.last_hidden_state  # shape: (batch_size, seq_len, hidden_size)\n",
    "\n",
    "# Mean pooling over tokens (ignore padding)\n",
    "attention_mask = inputs['attention_mask']\n",
    "input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n",
    "sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "sentence_embeddings = sum_embeddings / sum_mask\n",
    "\n",
    "# Now: sentence_embeddings is a torch.Tensor of shape (batch_size, 768)\n",
    "print(sentence_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Normalize vectors to unit length\n",
    "normed = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "# Compute cosine similarity matrix: (N x N)\n",
    "cos_sim_matrix = torch.matmul(normed, normed.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.9605e-08, 2.4579e-01],\n",
       "        [2.4579e-01, 2.3842e-07]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1- cos_sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, 13.4417],\n",
       "        [13.4417,  0.0000]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expand dimensions to broadcast: (N, 1, D) and (1, N, D)\n",
    "a = sentence_embeddings.unsqueeze(1)  # shape: (N, 1, D)\n",
    "b = sentence_embeddings.unsqueeze(0)  # shape: (1, N, D)\n",
    "\n",
    "# Compute pairwise Euclidean distances\n",
    "euclidean_dist_matrix = torch.sqrt(torch.sum((a - b) ** 2, dim=2))\n",
    "euclidean_dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.0000, 291.7520],\n",
       "        [291.7520,   0.0000]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manhattan_dist_matrix = torch.sum(torch.abs(a - b), dim=2)\n",
    "manhattan_dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
